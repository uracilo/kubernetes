Steps
================================================
Attach following Tag while creating resources.

Key:    kubernetes.io/cluster/kubernetes     
Value:  owned


==> Create a VPC with minimum 1 Public Subnet



==> Create an Security Groups


Create an Security Group for the Kubernetes Instances ( Eg: SG_K8S_CLUSTER_INSTANCES)

   Allow all connections from the VPC CIDR
   Allow 6443 from any Public IP from which you would like to manage the cluster



==> Create IAM Policies


kubernetes-master-node-policy



{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:DescribeTags",
        "ec2:DescribeInstances",
        "ec2:DescribeRegions",
        "ec2:DescribeRouteTables",
        "ec2:DescribeSecurityGroups",
        "ec2:DescribeSubnets",
        "ec2:DescribeVolumes",
        "ec2:CreateSecurityGroup",
        "ec2:CreateTags",
        "ec2:CreateVolume",
        "ec2:ModifyInstanceAttribute",
        "ec2:ModifyVolume",
        "ec2:AttachVolume",
        "ec2:AuthorizeSecurityGroupIngress",
        "ec2:CreateRoute",
        "ec2:DeleteRoute",
        "ec2:DeleteSecurityGroup",
        "ec2:DeleteVolume",
        "ec2:DetachVolume",
        "ec2:RevokeSecurityGroupIngress",
        "ec2:DescribeVpcs",
        "elasticloadbalancing:AddTags",
        "elasticloadbalancing:AttachLoadBalancerToSubnets",
        "elasticloadbalancing:ApplySecurityGroupsToLoadBalancer",
        "elasticloadbalancing:CreateLoadBalancer",
        "elasticloadbalancing:CreateLoadBalancerPolicy",
        "elasticloadbalancing:CreateLoadBalancerListeners",
        "elasticloadbalancing:ConfigureHealthCheck",
        "elasticloadbalancing:DeleteLoadBalancer",
        "elasticloadbalancing:DeleteLoadBalancerListeners",
        "elasticloadbalancing:DescribeLoadBalancers",
        "elasticloadbalancing:DescribeLoadBalancerAttributes",
        "elasticloadbalancing:DetachLoadBalancerFromSubnets",
        "elasticloadbalancing:DeregisterInstancesFromLoadBalancer",
        "elasticloadbalancing:ModifyLoadBalancerAttributes",
        "elasticloadbalancing:RegisterInstancesWithLoadBalancer",
        "elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer",
        "elasticloadbalancing:AddTags",
        "elasticloadbalancing:CreateListener",
        "elasticloadbalancing:CreateTargetGroup",
        "elasticloadbalancing:DeleteListener",
        "elasticloadbalancing:DeleteTargetGroup",
        "elasticloadbalancing:DescribeListeners",
        "elasticloadbalancing:DescribeLoadBalancerPolicies",
        "elasticloadbalancing:DescribeTargetGroups",
        "elasticloadbalancing:DescribeTargetHealth",
        "elasticloadbalancing:ModifyListener",
        "elasticloadbalancing:ModifyTargetGroup",
        "elasticloadbalancing:RegisterTargets",
        "elasticloadbalancing:SetLoadBalancerPoliciesOfListener",
        "iam:CreateServiceLinkedRole",
        "kms:DescribeKey"
      ],
      "Resource": [
        "*"
      ]
    }
  ]
}



kubernetes-worker-node-policy



{
      "Version": "2012-10-17",
      "Statement": [
          {
              "Effect": "Allow",
              "Action": [
                  "ec2:DescribeInstances",
                  "ec2:DescribeRegions",
                  "ecr:GetAuthorizationToken",
                  "ecr:BatchCheckLayerAvailability",
                  "ecr:GetDownloadUrlForLayer",
                  "ecr:GetRepositoryPolicy",
                  "ecr:DescribeRepositories",
                  "ecr:ListImages",
                  "ecr:BatchGetImage"
              ],
              "Resource": "*"
          } 
      ]
  }



==> Create IAM role for the EC2 instance


Create IAM roles for master and worker nodes.
For Each resource Setup AWS Tags, Resources used by the cluster must have specific AWS tags assigned to them.


Attach following Tag to

Key:    kubernetes.io/cluster/kubernetes     
Value:  owned

1. Create two Rhel Ec2 Instances. One for the master and the other for the worker node. With minimum requirements for the server and both master and worker nodes need to have at least 2 GB RAM and 2 CPUs.
you can take t2.medium type.


2. Log into both Master and Worker nodes over SSH.

3. Make sure the servers are up to date before installing anything new.

dnf -y upgrade


4. Disable SELinux enforcement.

setenforce 0
sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux

5. Enable transparent masquerading and facilitate Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster.

modprobe br_netfilter

6. Set bridged packets to traverse iptables rules.

cat < /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

Then load the new rules.

sysctl --system

7. Disable all memory swaps to increase performance.

swapoff -a
With these steps done on both Master and worker nodes, you can proceed to install Docker.

Installing Docker on Master and Worker nodes
Next, we’ll need to install Docker.

1. Add the repository for the docker installation package.

dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
2. Install container.io which is not yet provided by the package manager before installing docker.

dnf install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm
3. Then install Docker from the repositories.

dnf install docker-ce --nobest -y
4. Start the docker service.

systemctl start docker
5. Make it also start automatically on server restart.

systemctl enable docker
6. Change docker to use systemd cgroup driver.

echo '{
  "exec-opts": ["native.cgroupdriver=systemd"]
}' > /etc/docker/daemon.json
And restart docker to apply the change.

systemctl restart docker
Once installed, you should check that everything is working correctly.

7. See the docker version.

docker version


Now that Docker is ready to go, continue below to install Kubernetes itself.


Installing Kubernetes on Master and Worker nodes
With all the necessary parts installed, we can get Kubernetes installed as well.

1. Add the Kubernetes repository to your package manager by creating the following file.

cat < /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF


2. Then update the repo info.

dnf upgrade -y

3. Install all the necessary components for Kubernetes.

dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
Start the Kubernetes services and enable them to run at startup.

systemctl enable kubelet
systemctl start kubelet

Once running on both nodes, begin configuring Kubernetes on the Master by following the instructions in the next section.


================================================
Configuring Kubernetes on the Master node only
================================================
Once Kubernetes has been installed, it needs to be configured to form a cluster.

1. Configure kubeadm.

kubeadm config images pull

Issue the following command:

kubeadm init --pod-network-cidr 192.168.0.0/16


You should see something like the example below. Make note of the discovery token, it’s needed to join worker nodes to the cluster.

Note that the join token below is just an example.

kubeadm join 94.435.34.183:6443 --token 4xrp9o.v345aic7zc1bj8ba 
--discovery-token-ca-cert-hash sha256:b2e459930f030787654489ba7ccbc701c29b3b60e0aa4998706fe0052de8794c


Make the following directory and configuration files.


mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config


2. Install CNI (container network interface) plugin for Kubernetes.

kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

3. Check that Master node has been enabled and is running.

kubectl get nodes


On successful execution, you should see a node with ready status. If not, wait a moment and repeat the command.

When the Master node is up and running, continue with the next section to join the Worker node to the cluster.


================================================
Join the  Worker node to cluster
================================================

Join the cluster with the previously noted token.

Note that the join token below is just an example.

kubeadm join 94.435.34.183:6443 --token 4xrp9o.v345aic7zc1bj8ba 
--discovery-token-ca-cert-hash sha256:b2e459930f030787654489ba7ccbc701c29b3b60e0aa4998706fe0052de8794c

See if the Worker node successfully joined.

Go back to the Master node and issue the following command.

kubectl get nodes

On success, you should see two nodes with ready status. If not, wait a moment and repeat the command.





